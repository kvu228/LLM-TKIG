# Training Configuration
training:
  # Model parameters
  model_name: "meta-llama/Llama-2-7b-hf"
  max_length: 512
  batch_size: 8
  gradient_accumulation_steps: 4
  
  # LoRA parameters
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  
  # Training parameters
  learning_rate: 2e-4
  num_train_epochs: 3
  warmup_steps: 100
  weight_decay: 0.01
  
  # Task-specific parameters
  task_type: "all"  # 'all', 'topic', 'entity', 'ttp'
  
  # Output and logging
  output_dir: "outputs"
  logging_dir: "logs"
  logging_steps: 10
  evaluation_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 100
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss" 