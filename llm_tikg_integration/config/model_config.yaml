# Model Configuration
model:
  # Base model
  name: "meta-llama/Llama-2-7b-hf"
  revision: "main"
  trust_remote_code: true
  
  # Model parameters
  max_length: 512
  use_cache: false
  torch_dtype: "bfloat16"
  
  # LoRA configuration
  lora:
    r: 16
    alpha: 32
    dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
    bias: "none"
    task_type: "CAUSAL_LM"
  
  # Quantization (optional)
  quantization:
    load_in_8bit: false
    load_in_4bit: true
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: "nf4" 